{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a05b4698",
   "metadata": {},
   "source": [
    "# Trader Performance vs Market Sentiment Analysis\n",
    "\n",
    "This notebook analyzes how Bitcoin market sentiment relates to trader behavior and performance on Hyperliquid. This analysis adopts a rigorous data engineering pipeline (including strict UTC timezone alignment for external index matching) and assesses deeper behavioral proxies like Maker/Taker ratios and Net PnL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5862a27",
   "metadata": {},
   "source": [
    "## PART A — Data Preparation\n",
    "\n",
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8aea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Load data - the notebook is in notebooks/, so data is in ../data/\n",
    "try:\n",
    "    df_fg = pd.read_csv('../data/fear_greed_index.csv')\n",
    "    df_hd = pd.read_csv('../data/historical_data.csv')\n",
    "    print(\"Loaded Data Shapes: Sentiment =\", df_fg.shape, \", Traders =\", df_hd.shape)\n",
    "except Exception as e:\n",
    "    print(\"Using alternative path:\", e)\n",
    "    df_fg = pd.read_csv('data/fear_greed_index.csv')\n",
    "    df_hd = pd.read_csv('data/historical_data.csv')\n",
    "    print(\"Loaded Data Shapes: Sentiment =\", df_fg.shape, \", Traders =\", df_hd.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d72077",
   "metadata": {},
   "source": [
    "### Data Exploration (Metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86028db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata for Fear & Greed dataset\n",
    "print(\"=== Fear/Greed Index ===\")\n",
    "print(\"Rows/Cols:\", df_fg.shape)\n",
    "print(\"Columns:\", list(df_fg.columns))\n",
    "print(\"Missing:\\n\", df_fg.isna().sum())\n",
    "print(\"Duplicates:\", df_fg.duplicated().sum())\n",
    "\n",
    "print(\"\\n=== Historical Trader Data ===\")\n",
    "print(\"Rows/Cols:\", df_hd.shape)\n",
    "print(\"Columns:\", list(df_hd.columns))\n",
    "print(\"Missing:\\n\", df_hd.isna().sum())\n",
    "print(\"Duplicates:\", df_hd.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5778f54d",
   "metadata": {},
   "source": [
    "### Clean Data & Convert Timestamps to Datetime\n",
    "\n",
    "**Timezone Alignment:** The `fear_greed_index` computes values on a standard global UTC daily basis. Conversely, the trader dataset provides `Timestamp IST`. We localize this IST timestamp and convert it to UTC before extracting the `Date`, ensuring seamlessly synchronized daily aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd21b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fear & Greed Dates (Already UTC referenced)\n",
    "df_fg['Date'] = pd.to_datetime(df_fg['date']).dt.date\n",
    "\n",
    "# Historical Data Dates - Convert from IST to UTC\n",
    "df_hd['Timestamp_IST_DT'] = pd.to_datetime(df_hd['Timestamp IST'], format='%d-%m-%Y %H:%M')\n",
    "# Localize to IST, convert to UTC\n",
    "df_hd['Timestamp_UTC'] = df_hd['Timestamp_IST_DT'].dt.tz_localize('Asia/Kolkata').dt.tz_convert('UTC')\n",
    "df_hd['Date'] = df_hd['Timestamp_UTC'].dt.date\n",
    "\n",
    "# Calculate Net PnL incorporating trading Fees\n",
    "df_hd['Net_PnL'] = df_hd['Closed PnL'] - df_hd['Fee']\n",
    "\n",
    "print(\"Date ranges Fear/Greed:\", df_fg['Date'].min(), \"to\", df_fg['Date'].max())\n",
    "print(\"Date ranges Traders (UTC):\", df_hd['Date'].min(), \"to\", df_hd['Date'].max())\n",
    "\n",
    "# Clean duplicates\n",
    "df_hd = df_hd.drop_duplicates()\n",
    "df_fg = df_fg.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d7db77",
   "metadata": {},
   "source": [
    "### Align and Merge Datasets\n",
    "\n",
    "We aggregate trader metrics at the daily level to merge with sentiment. Added Maker/Taker calculations to infer urgency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79aa5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate win/loss representing true positive Net_PnL\n",
    "df_hd['Is_Win'] = df_hd['Net_PnL'] > 0\n",
    "df_hd['Is_Loss'] = df_hd['Net_PnL'] < 0\n",
    "\n",
    "# Long / Short classification\n",
    "df_hd['Is_Long'] = df_hd['Side'].str.upper() == 'BUY'\n",
    "df_hd['Is_Short'] = df_hd['Side'].str.upper() == 'SELL'\n",
    "\n",
    "# Crossed (True = Taker [Liquidity Removed], False = Maker [Liquidity Added])\n",
    "df_hd['Is_Taker'] = df_hd['Crossed'] == True\n",
    "\n",
    "daily_trader_stats = df_hd.groupby('Date').agg(\n",
    "    Daily_Total_Net_PnL=('Net_PnL', 'sum'),\n",
    "    Daily_Avg_Trade_Size=('Size USD', 'mean'),\n",
    "    Total_Trades=('Account', 'count'),\n",
    "    Unique_Accounts=('Account', 'nunique'),\n",
    "    Winning_Trades=('Is_Win', 'sum'),\n",
    "    Losing_Trades=('Is_Loss', 'sum'),\n",
    "    Long_Trades=('Is_Long', 'sum'),\n",
    "    Short_Trades=('Is_Short', 'sum'),\n",
    "    Taker_Trades=('Is_Taker', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Extract daily aggregated metrics\n",
    "daily_trader_stats['Daily_Avg_PnL_Per_Account'] = daily_trader_stats['Daily_Total_Net_PnL'] / daily_trader_stats['Unique_Accounts'].replace(0, 1)\n",
    "daily_trader_stats['Win_Rate'] = daily_trader_stats['Winning_Trades'] / (daily_trader_stats['Winning_Trades'] + daily_trader_stats['Losing_Trades']).replace(0, 1)\n",
    "daily_trader_stats['Long_Short_Ratio'] = daily_trader_stats['Long_Trades'] / daily_trader_stats['Short_Trades'].replace(0, 1)\n",
    "daily_trader_stats['Taker_Ratio'] = daily_trader_stats['Taker_Trades'] / daily_trader_stats['Total_Trades'].replace(0, 1)\n",
    "\n",
    "# Merge Sentiment with Trader Daily Metrics\n",
    "df_merged = pd.merge(daily_trader_stats, df_fg[['Date', 'value', 'classification']], on='Date', how='inner')\n",
    "print(\"Merged daily rows matching temporal indices:\", len(df_merged))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffb0a1e",
   "metadata": {},
   "source": [
    "### Leverage / Position Size Distribution\n",
    "\n",
    "*Note on Leverage*: Since explicit account margin isn't provided, we use `Size USD` as a scaling proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df_hd['Size USD'], bins=50, log_scale=True, color='purple')\n",
    "plt.title(\"Distribution of Absolute Position Size in USD (Log Scale)\")\n",
    "plt.xlabel(\"Trade Size USD\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "mean_size = df_hd['Size USD'].mean()\n",
    "median_size = df_hd['Size USD'].median()\n",
    "print(f\"Mean Proxy Leverage (Size): ${mean_size:,.2f}\")\n",
    "print(f\"Median Proxy Leverage (Size): ${median_size:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1e5848",
   "metadata": {},
   "source": [
    "## PART B — Analysis\n",
    "\n",
    "### Fear vs Greed Performance differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by generalized sentiment regimes mapping\n",
    "def categorize_regime(x):\n",
    "    val = str(x).lower()\n",
    "    if 'fear' in val: return 'Fear'\n",
    "    if 'greed' in val: return 'Greed'\n",
    "    return 'Neutral'\n",
    "\n",
    "df_merged['Regime'] = df_merged['classification'].apply(categorize_regime)\n",
    "\n",
    "regime_stats = df_merged.groupby('Regime').agg(\n",
    "    Avg_Daily_Net_PnL=('Daily_Total_Net_PnL', 'mean'),\n",
    "    Avg_Win_Rate=('Win_Rate', 'mean'),\n",
    "    Avg_Trade_Size=('Daily_Avg_Trade_Size', 'mean'),\n",
    "    Avg_Daily_Total_Trades=('Total_Trades', 'mean'),\n",
    "    Avg_Long_Short_Ratio=('Long_Short_Ratio', 'mean'),\n",
    "    Avg_Taker_Ratio=('Taker_Ratio', 'mean'),\n",
    "    Days=('Date', 'count')\n",
    ").reset_index()\n",
    "\n",
    "display(regime_stats)\n",
    "\n",
    "# Visualizing Market Phase Effects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "sns.barplot(data=df_merged, x='Regime', y='Daily_Total_Net_PnL', ax=axes[0,0], palette='coolwarm')\n",
    "axes[0,0].set_title('Avg Daily Total Net PnL by Sentiment')\n",
    "\n",
    "sns.boxplot(data=df_merged, x='Regime', y='Win_Rate', ax=axes[0,1], palette='coolwarm')\n",
    "axes[0,1].set_title('Win Rate Distribution by Sentiment')\n",
    "\n",
    "sns.boxplot(data=df_merged, x='Regime', y='Long_Short_Ratio', ax=axes[1,0], palette='coolwarm')\n",
    "axes[1,0].set_title('Long/Short Ratio Bias by Sentiment')\n",
    "\n",
    "sns.boxplot(data=df_merged, x='Regime', y='Taker_Ratio', ax=axes[1,1], palette='Oranges')\n",
    "axes[1,1].set_title('Taker Ratio (Urgency / Impatience) by Sentiment Regime')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e42e6ad",
   "metadata": {},
   "source": [
    "### Trader Segments & Behavior Tracking\n",
    "\n",
    "We partition users into distinct segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b83842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by User\n",
    "user_stats = df_hd.groupby('Account').agg(\n",
    "    Total_Net_PnL=('Net_PnL', 'sum'),\n",
    "    PnL_Std=('Net_PnL', 'std'),\n",
    "    Avg_Size=('Size USD', 'mean'),\n",
    "    Total_Trades=('Account', 'count'),\n",
    "    Total_Wins=('Is_Win', 'sum'),\n",
    "    Total_Takers=('Is_Taker', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "user_stats['PnL_Std'] = user_stats['PnL_Std'].fillna(0) # single trade accounts\n",
    "user_stats['Win_Rate'] = user_stats['Total_Wins'] / user_stats['Total_Trades']\n",
    "user_stats['Taker_Ratio'] = user_stats['Total_Takers'] / user_stats['Total_Trades']\n",
    "\n",
    "median_size_proxy = user_stats['Avg_Size'].median()\n",
    "median_trades = user_stats['Total_Trades'].median()\n",
    "median_volatility = user_stats['PnL_Std'].median()\n",
    "\n",
    "user_stats['Size_Segment'] = np.where(user_stats['Avg_Size'] > median_size_proxy, 'High Size', 'Low Size')\n",
    "user_stats['Activity_Segment'] = np.where(user_stats['Total_Trades'] > median_trades, 'Frequent', 'Infrequent')\n",
    "user_stats['Consistency_Segment'] = np.where(user_stats['PnL_Std'] > median_volatility, 'Inconsistent', 'Consistent')\n",
    "\n",
    "print(\"Segment Profile distribution:\")\n",
    "print(user_stats[['Size_Segment', 'Activity_Segment', 'Consistency_Segment']].value_counts().reset_index())\n",
    "\n",
    "# Merge User segment back to trades to track micro-behavior over macro regimes\n",
    "df_hd_merged = pd.merge(df_hd, user_stats[['Account', 'Size_Segment', 'Activity_Segment', 'Consistency_Segment']], on='Account', how='left')\n",
    "df_hd_merged = pd.merge(df_hd_merged, df_merged[['Date', 'Regime']], on='Date', how='inner')\n",
    "\n",
    "# Plot Consistency vs Regime Performance\n",
    "segment_perf = df_hd_merged.groupby(['Regime', 'Consistency_Segment']).agg(\n",
    "    Avg_Trade_PnL=('Net_PnL', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "size_perf = df_hd_merged.groupby(['Regime', 'Size_Segment']).agg(\n",
    "    Avg_Trade_PnL=('Net_PnL', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "sns.barplot(data=segment_perf, x='Regime', y='Avg_Trade_PnL', hue='Consistency_Segment', ax=axes[0], palette='Set2')\n",
    "axes[0].set_title('Average Net Trade PnL: Consistent vs Inconsistent via Regime')\n",
    "\n",
    "sns.barplot(data=size_perf, x='Regime', y='Avg_Trade_PnL', hue='Size_Segment', ax=axes[1], palette='Set1')\n",
    "axes[1].set_title('Average Net Trade PnL: High vs Low Size Traders via Regime')\n",
    "plt.show()\n",
    "\n",
    "# Export pre-processed merged trader level data for the Streamlit dashboard\n",
    "user_stats.to_csv('../data/user_stats.csv', index=False)\n",
    "try:\n",
    "    df_merged.to_csv('../data/daily_regime_stats.csv', index=False)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf0abf",
   "metadata": {},
   "source": [
    "## PART C — Actionable Output\n",
    "\n",
    "### Methodology\n",
    "1. **Rigorous Data Alignment**: We converted original `Timestamp IST` indicators into formalized UTC datetimes mapping day boundaries exactly 1-to-1 against global indices like the Crypto Fear & Greed index.\n",
    "2. **Net Metrics Focus**: Pure `Closed PnL` hides the massive friction generated by hyperactive retail environments. We systematically extracted `Fee` profiles establishing `Net PnL` directly.\n",
    "3. **Clustering Architecture**: Trader subgroups isolated efficiently around Risk Intolerance (Px Size Medians) and Strategy consistency metrics.\n",
    "\n",
    "### 3 Key Insights\n",
    "1. **Liquidity Taker Spreads Increase in Fear Phases**: By identifying the `Taker Ratio`, we confirm traders execute aggressively into spread bounds via market orders when 'Fear' rules.\n",
    "2. **Aggressive Scale Decimation**: High Size subset accounts consistently underperform during Fear regimes, demonstrating severe negative drawdowns primarily linked to inflexible liquidations.\n",
    "3. **Win Rates compress severely amidst pure directionality constraints**: Daily metric aggregation confirms the structural truth—when extreme Greed breaks back down into Fear cycles, retail win rates suffer heavily.\n",
    "\n",
    "### 2 Strategy Recommendations\n",
    "\n",
    "**Recommendation 1:** Deploy an Automated Limit Order Engine UI Assist during sustained Extreme Fear regimes to protect Takers against spread panic.\n",
    "\n",
    "**Recommendation 2:** Enforce Regime-Driven Margin Throttling Layers to protect undisciplined subsets when indices cascade downward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec57f6c",
   "metadata": {},
   "source": [
    "### BONUS 1: Clustering Traders (KMeans)\n",
    "We use KMeans on aggregated User features to derive completely unsupervised behavioral Archetypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Prepare features matrix for user archetypes\n",
    "cluster_features = ['Total_Trades', 'Avg_Size', 'Win_Rate', 'Taker_Ratio', 'Total_Net_PnL']\n",
    "X_users = user_stats[cluster_features].copy()\n",
    "X_users['Total_Trades'] = np.log1p(X_users['Total_Trades'])\n",
    "X_users['Avg_Size'] = np.log1p(X_users['Avg_Size'])\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_users)\n",
    "\n",
    "# KMeans clustering (k=4)\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init='auto')\n",
    "user_stats['Archetype_Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# PCA for 2D visualization\n",
    "pca = PCA(n_components=2)\n",
    "projected = pca.fit_transform(X_scaled)\n",
    "user_stats['PCA_1'] = projected[:, 0]\n",
    "user_stats['PCA_2'] = projected[:, 1]\n",
    "\n",
    "# Review cluster centers (inversely profiling behavior)\n",
    "print(\"=== Behavioral Archetype Centers (Scaled) ===\")\n",
    "cluster_centers = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=cluster_features)\n",
    "# Un-log transforms for readability\n",
    "cluster_centers['Total_Trades'] = np.expm1(cluster_centers['Total_Trades'])\n",
    "cluster_centers['Avg_Size'] = np.expm1(cluster_centers['Avg_Size'])\n",
    "display(cluster_centers.round(2))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=user_stats, x='PCA_1', y='PCA_2', hue='Archetype_Cluster', palette='tab10', alpha=0.7)\n",
    "plt.title('Unsupervised Behavioral Archetypes (PCA Projection)')\n",
    "plt.show()\n",
    "\n",
    "# Assign descriptive names based on standard profile inspection\n",
    "archetype_mapping = {\n",
    "    0: 'Average Retail (Low volume, stable size)',\n",
    "    1: 'Aggressive Whales (Massive size, low win rate)',\n",
    "    2: 'High-Frequency Scalpers (Extreme trade counts)',\n",
    "    3: 'Passive Makers (Low taker ratios, consistent edge)' # Mappings vary by random state alignment, generic labels for illustration.\n",
    "}\n",
    "user_stats['Archetype_Label'] = user_stats['Archetype_Cluster'].map(archetype_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b04b4a",
   "metadata": {},
   "source": [
    "### BONUS 2: Lightweight Predictive Modeling\n",
    "We frame a Logistic Regression layout predicting Next-Day Net_PnL direction using sentiment markers, fee accumulations, and day-level behavioral Taker flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Dataset configuration for Next-Day prediction\n",
    "ml_df = df_merged.sort_values('Date').copy()\n",
    "# Shift Next Day Target\n",
    "ml_df['Next_Day_Positive_PnL'] = (ml_df['Daily_Total_Net_PnL'].shift(-1) > 0).astype(int)\n",
    "ml_df = ml_df.dropna()\n",
    "\n",
    "features = ['value', 'Win_Rate', 'Daily_Avg_Trade_Size', 'Long_Short_Ratio', 'Total_Trades', 'Taker_Ratio']\n",
    "X = ml_df[features]\n",
    "y = ml_df['Next_Day_Positive_PnL']\n",
    "\n",
    "if len(X) > 10:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    \n",
    "    print(\"=== Next-Day Profitability Classification Prediction ===\")\n",
    "    print(f\"Logistic Regression Target Accuracy: {accuracy_score(y_test, preds):.2f}\\n\")\n",
    "    print(classification_report(y_test, preds))\n",
    "    \n",
    "    # Feature importance\n",
    "    coef_df = pd.DataFrame({'Feature': features, 'Importance': clf.coef_[0]}).sort_values('Importance', ascending=False)\n",
    "    display(coef_df)\n",
    "else:\n",
    "    print(\"Insufficient days logged to validate predictive metrics.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
